{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbeuifLTd1Gs",
        "outputId": "1ffe540f-5ee4-41d4-b3a7-74b00fd3e684"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qOrF-GUxWVU"
      },
      "outputs": [],
      "source": [
        "token = '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'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-8wSJNVw_qK"
      },
      "source": [
        "# data and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGu4rmesYQu5"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/drive/My Drive/nlp datasets/' + 'prompt_train.csv'\n",
        "test_path = '/content/drive/My Drive/nlp datasets/' + 'prompt_test.csv'\n",
        "train = pd.read_csv(train_path)\n",
        "test  = pd.read_csv(test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgFEpfypfSxO"
      },
      "outputs": [],
      "source": [
        "prompt_train = list(train['prompt'])\n",
        "prompt_test = list(test['prompt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSnMXbcTbK-n"
      },
      "outputs": [],
      "source": [
        "!pip install -U \"huggingface_hub[cli]\"\n",
        "!huggingface-cli login\n",
        "!huggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --exclude \"original/*\" --local-dir meta-llama/Meta-Llama-3-8B-Instruct\n",
        "!pip install -q accelerate transformers bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eRtIJaLDdifK",
        "outputId": "a81d6db4-3cba-4b84-da18-f37309b72c08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vgnR3xLdne3"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIqnSDXHb7wP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "8c1633f65dc84416b5a03ec818021fe4",
            "29338587f63b41718975fc3a5bbbfb40",
            "54efb954ebec48419423e7b87aa63faf",
            "6499c9ff55e24bfdbbba7f90a3de2818",
            "b9e56d37ef3e4e6e91d728073a9a085b",
            "f5cb35f27ae94fd4a6b80f1077478058",
            "dcccfe1dabb64bdf81713a9f0cb4796d",
            "162062d3c25a4fa2b39e2945c560aba7",
            "87434019c5a542cca75044b0bd94c1ac",
            "d3b90bec1970431cbfed638815bf43ca",
            "fe1afd8ec0c1491997ea621953031ac3"
          ]
        },
        "outputId": "dfe7798b-94d4-4ef5-b951-935aa5f2441c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c1633f65dc84416b5a03ec818021fe4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/meta-llama/Meta-Llama-3-8B-Instruct\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"/content/meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "     device_map=\"auto\",\n",
        "     torch_dtype='auto',\n",
        "     low_cpu_mem_usage=True\n",
        " )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYfybQ2TwBKd"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "import torch\n",
        "# Function to generate responses for a batch of prompts\n",
        "def generate_responses(prompts):\n",
        "    # Step 3: Tokenize the prompts\n",
        "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "    # Step 4: Generate responses\n",
        "    with torch.no_grad():  # Disable gradient calculation for inference\n",
        "        outputs = model.generate(**inputs, max_length=max_length, num_return_sequences=1)\n",
        "\n",
        "    # Step 5: Decode the responses\n",
        "    responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    return responses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxhkGaalmj4u"
      },
      "source": [
        "# wiki"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GraeE_qxh0Xl",
        "outputId": "7f9133ce-8284-4502-e7d4-f8f97ca610b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia-api wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dO18Nw3kosw",
        "outputId": "816a2dd4-d168-4992-9e81-60043fe0d498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevant Background Data from Wikipedia:\n",
            "Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\n",
            "Some high-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT, and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\n",
            "The various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence—the ability to complete any task performable by a human on an at least equal level—is among the field's long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.\n",
            "Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism, followed by periods of disappointment and loss of funding, known as\n"
          ]
        }
      ],
      "source": [
        "import wikipedia\n",
        "import wikipediaapi\n",
        "\n",
        "# Set a proper user agent for Wikipedia requests\n",
        "user_agent = 'MyWikipediaBot/0.1 (aminghassemi7@gmail.com)'\n",
        "wiki_wiki = wikipediaapi.Wikipedia(user_agent=user_agent)\n",
        "\n",
        "# Function to search Wikipedia based on a query (the question)\n",
        "def search_wikipedia(query, num_results=3):\n",
        "    try:\n",
        "        search_results = wikipedia.search(query, results=num_results)\n",
        "        retrieved_docs = []\n",
        "\n",
        "        for result in search_results:\n",
        "            # Use wikipediaapi to get the page text\n",
        "            page = wiki_wiki.page(result)\n",
        "            if page.exists():\n",
        "                retrieved_docs.append(page.text)\n",
        "\n",
        "        if not retrieved_docs:\n",
        "            print(f\"No relevant Wikipedia pages found for: {query}\")\n",
        "            return []\n",
        "\n",
        "        return retrieved_docs\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error while searching Wikipedia: {e}\")\n",
        "        return []\n",
        "\n",
        "# Function to retrieve relevant Wikipedia background information based on the question\n",
        "def retrieve_background_data(question, num_results=3):\n",
        "    # Search Wikipedia based on the question query\n",
        "    documents = search_wikipedia(question, num_results=num_results)\n",
        "\n",
        "    if not documents:\n",
        "        print(f\"No relevant Wikipedia pages found for: {question}\")\n",
        "        return \"\"\n",
        "\n",
        "    # Combine the retrieved pages into one background document\n",
        "    background_data = \" \".join(documents)\n",
        "\n",
        "    return background_data\n",
        "\n",
        "# Example question\n",
        "# Make sure prompt_test is defined\n",
        "prompt_test = [\"What is artificial intelligence?\", \"Explain quantum computing\", \"Who is Albert Einstein?\"]\n",
        "question = prompt_test[0]\n",
        "\n",
        "# Retrieve the relevant background data from Wikipedia\n",
        "background_data = retrieve_background_data(question)\n",
        "\n",
        "# Output the retrieved background data\n",
        "print(\"Relevant Background Data from Wikipedia:\")\n",
        "print(background_data[:2000])  # Display first 2000 characters for brevity\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_test[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qrxR7EQJ3uWD",
        "outputId": "d5407d44-6202-4c4b-ecac-0988f96a0a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is artificial intelligence?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Qou1_N5wsTx"
      },
      "outputs": [],
      "source": [
        "prompt_wiki_test = [retrieve_background_data(question) for question in prompt_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1su0zeYmm2K"
      },
      "source": [
        "# most relevant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSlYiH9JntBD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c187fbfb-de1c-4590-a86d-41b9f867ca61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Relevant Sentences:\n",
            "- Artificial Intelligence.\n",
            "- \"Logic and Artificial Intelligence\".\n",
            "- He proposed a distinction between two hypotheses about artificial intelligence:\n",
            "\n",
            "Strong AI hypothesis: An artificial intelligence system can have \"a mind\" and \"consciousness\".\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Make sure to download the necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to split a long document into sentences\n",
        "def split_document_into_sentences(document):\n",
        "    sentences = nltk.sent_tokenize(document)\n",
        "    return sentences\n",
        "\n",
        "# Function to retrieve the most relevant sentences based on the query\n",
        "def retrieve_relevant_sentences(query, document, top_k=5):\n",
        "    # Split the document into sentences\n",
        "    sentences = split_document_into_sentences(document)\n",
        "\n",
        "    # Create a TF-IDF vectorizer\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "    # Combine the query and sentences for TF-IDF processing\n",
        "    documents = [query] + sentences\n",
        "\n",
        "    # Fit and transform the documents using TF-IDF\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "    # Calculate the cosine similarity of the query with each sentence\n",
        "    query_vector = tfidf_matrix[0]\n",
        "    sentence_vectors = tfidf_matrix[1:]\n",
        "\n",
        "    # Calculate cosine similarities\n",
        "    cosine_similarities = (sentence_vectors * query_vector.T).toarray().flatten()\n",
        "\n",
        "    # Get the indices of the top_k most similar sentences\n",
        "    relevant_indices = np.argsort(cosine_similarities)[-top_k:][::-1]\n",
        "\n",
        "    # Retrieve and return the most relevant sentences\n",
        "    relevant_sentences = [sentences[i] for i in relevant_indices]\n",
        "\n",
        "    return relevant_sentences\n",
        "\n",
        "# Example long document\n",
        "long_document = prompt_wiki_test[0]\n",
        "\n",
        "# Example query\n",
        "query = prompt_test[0]\n",
        "\n",
        "# Retrieve the most relevant sentences\n",
        "relevant_sentences = retrieve_relevant_sentences(query, long_document, top_k=3)\n",
        "\n",
        "# Output the relevant sentences\n",
        "print(\"Most Relevant Sentences:\")\n",
        "for sentence in relevant_sentences:\n",
        "    print(f\"- {sentence}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puvZM69SntuM"
      },
      "source": [
        "# llama prompting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_a = list(train['A'])\n",
        "train_b = list(train['B'])\n",
        "train_c = list(train['C'])\n",
        "train_d = list(train['D'])\n",
        "train_e = list(train['E'])"
      ],
      "metadata": {
        "id": "8ANagDVy4DTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ans = train['answer']"
      ],
      "metadata": {
        "id": "TPuFXX4H6V3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_a[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mns1h37R5et_",
        "outputId": "a59af3b6-7433-4bef-82e2-0242e9f0866c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The main sequence is a type of galaxy that contains a large number of stars.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_a = list(test['A'])\n",
        "test_b = list(test['B'])\n",
        "test_c = list(test['C'])\n",
        "test_d = list(test['D'])\n",
        "test_e = list(test['E'])"
      ],
      "metadata": {
        "id": "DZdOlAyU4S9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0iGYsTwY45yz",
        "outputId": "0da3e7fd-330d-434c-e81f-21ac00ea8f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is the main sequence in astronomy?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(i):\n",
        "  inst = \"For the following question, return alphabet of correct answer from choices: (A, B, C, D, E).\\n\"\n",
        "  q = prompt_test[i] + '\\n'\n",
        "  choices = 'A:' + test_a[i] +'\\n' + 'B:' + test_b[i]+ '\\n'+ 'C:' + test_c[i]+ '\\n'+ 'D:' + test_d[i]+ '\\n'+ 'E:' + test_e[i]+ '\\n'\n",
        "  s = prompt_wiki_test[i] +'\\n' + inst + q + choices\n",
        "  return s"
      ],
      "metadata": {
        "id": "tF83h2Bc7e9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instructs = [sample(i) for i in range(len(prompt_test))]"
      ],
      "metadata": {
        "id": "te0td4ZZ8f0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6yn9lepnVpk"
      },
      "outputs": [],
      "source": [
        "def generate_responses(prompts):\n",
        "    # Step 3: Tokenize the prompts\n",
        "    inputs = [tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device) for prompt in prompts]\n",
        "\n",
        "    ans = []\n",
        "\n",
        "    # Step 4: Generate responses\n",
        "    with torch.no_grad():  # Disable gradient calculation for inference\n",
        "        for input in inputs:\n",
        "\n",
        "\n",
        "    # Step 5: Decode the responses\n",
        "    responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    return responses\n",
        "\n",
        "# Generate responses for the batch\n",
        "responses = generate_responses(instructs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gMXxMI8o9oDH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "l-8wSJNVw_qK"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8c1633f65dc84416b5a03ec818021fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29338587f63b41718975fc3a5bbbfb40",
              "IPY_MODEL_54efb954ebec48419423e7b87aa63faf",
              "IPY_MODEL_6499c9ff55e24bfdbbba7f90a3de2818"
            ],
            "layout": "IPY_MODEL_b9e56d37ef3e4e6e91d728073a9a085b"
          }
        },
        "29338587f63b41718975fc3a5bbbfb40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5cb35f27ae94fd4a6b80f1077478058",
            "placeholder": "​",
            "style": "IPY_MODEL_dcccfe1dabb64bdf81713a9f0cb4796d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "54efb954ebec48419423e7b87aa63faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_162062d3c25a4fa2b39e2945c560aba7",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87434019c5a542cca75044b0bd94c1ac",
            "value": 4
          }
        },
        "6499c9ff55e24bfdbbba7f90a3de2818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3b90bec1970431cbfed638815bf43ca",
            "placeholder": "​",
            "style": "IPY_MODEL_fe1afd8ec0c1491997ea621953031ac3",
            "value": " 4/4 [00:59&lt;00:00, 11.20s/it]"
          }
        },
        "b9e56d37ef3e4e6e91d728073a9a085b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5cb35f27ae94fd4a6b80f1077478058": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcccfe1dabb64bdf81713a9f0cb4796d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "162062d3c25a4fa2b39e2945c560aba7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87434019c5a542cca75044b0bd94c1ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3b90bec1970431cbfed638815bf43ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe1afd8ec0c1491997ea621953031ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}